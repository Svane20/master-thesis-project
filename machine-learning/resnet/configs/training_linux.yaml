# @package _global_

# Data augmentation configuration
transforms:
  train:
    RandomAffine:
      degrees: 30
      translate: null
      scale: [ 0.8, 1.25 ]
      shear: 10
      flip: 0.5
      resample: false
      fillcolor: 0
    TopBiasedRandomCrop:
      output_size: [ 512, 512 ]
      top_crop_ratio: 0.4
      low_threshold: 0.1
      high_threshold: 0.9
    RandomJitter:
      enabled: true # Hack to show in wandb
    ToTensor:
      enabled: true # Hack to show in wandb
    Rescale:
      scale: 0.00392156862745098 # 1/255
    Normalize:
      mean: [ 0.485, 0.456, 0.406 ]
      std: [ 0.229, 0.224, 0.225 ]
      inplace: false
  val:
    OriginScale:
      output_size: 512
    ToTensor:
      enabled: true # Hack to show in wandb
    Rescale:
      scale: 0.00392156862745098 # 1/255
    Normalize:
      mean: [ 0.485, 0.456, 0.406 ]
      std: [ 0.229, 0.224, 0.225 ]
      inplace: false

# Dataset configuration
dataset:
  name: "synthetic-data"
  root: "/mnt/shared/datasets/processed"
  batch_size: 8
  pin_memory: true
  train:
    num_workers: 4
    shuffle: true
    drop_last: true
    use_trimap: false
    use_composition: false
  val:
    num_workers: 4
    shuffle: false
    drop_last: false

# Training configuration
training:
  max_epochs: 100
  warmup_epochs: 2
  accelerator: cuda
  seed: 42
  compile_model: true

  # Criterion configuration
  criterion:
    losses: [ "l1_loss", "gradient_loss", "laplacian_pha_loss" ]
    normalize_weights: true
    weight_dict:
      l1_loss: 1.0
      gradient_loss: 0.50
      laplacian_pha_loss: 0.7

  # Optimizer configuration
  optimizer:
    name: "AdamW"
    lr: 4e-4
    weight_decay: 1.0e-7
    amp:
      enabled: true
      amp_dtype: float16
    gradient_clip:
      enabled: true
      max_norm: 0.1
      norm_type: 2

  # Scheduler configuration
  scheduler:
    name: "CosineAnnealingLR"
    enabled: true
    parameters:
      eta_min: 1.0e-7

  # Early stopping configuration
  early_stopping:
    enabled: true
    verbose: true
    patience: 30
    min_delta: 0.0
    monitor: "mae"
    mode: "min"

  # Logging configuration
  logging:
    wandb:
      enabled: true
      project: "U-NET"
      entity: "svane20-keyshot"
      tags: [ "unet", "resnet-50", "synthetic-data" ]
      notes: "Training with ResNet-50 backbone on synthetic data."
      group: "resnet_50"
      job_type: "training"
    log_metrics: false
    log_freq: 10
    log_images_freq: 10
    image_log_count: 8

  # Checkpoint configuration
  checkpoint:
    save_directory: "/mnt/shared/ml/checkpoints/resnet"
    save_freq: 5
    checkpoint_path: "resnet_50_512_v6.pt"
    resume_from: null # "/mnt/shared/ml/checkpoints/resnet/<CHECKPOINT>" or null

# Model configuration

# ResNet-34
#model:
#  model_name: "ResNet34Matte"
#  encoder:
#    pretrained: true
#  decoder:
#    encoder_channels: [ 64, 64, 128, 256, 512 ]
#    decoder_channels: [ 256, 128, 64, 64 ]
#    final_channels: 64

# ResNet-50
model:
  model_name: "ResNet50Matte"
  encoder:
    pretrained: true
  decoder:
    encoder_channels: [ 64, 256, 512, 1024, 2048 ]
    decoder_channels: [ 512, 256, 128, 64 ]
    final_channels: 64