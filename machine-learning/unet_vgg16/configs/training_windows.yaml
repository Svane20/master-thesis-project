# @package _global_

# Global configuration
scratch:
  resolution: 512

# Dataset configuration
dataset:
  name: "synthetic-data"
  root: "D:/OneDrive/Master Thesis/datasets/processed"
  batch_size: 10
  pin_memory: true
  train:
    num_workers: 4
    shuffle: true
    drop_last: true
  test:
    num_workers: 4
    shuffle: false
    drop_last: false

# Training configuration
training:
  max_epochs: 200
  warmup_epochs: 20 # 5-10% of max_epochs
  accelerator: cuda
  seed: 42
  compile_model: false

  # Criterion configuration
  criterion:
    name: "MattingLossV2"
    weight_dict:
      boundary: 0.5
      composition: 1.0
      gradient: 0.5
      l1: 1.0
      laplacian: 1.0

  # Optimizer configuration
  optimizer:
    name: "AdamW"
    lr: 3e-4
    weight_decay: 1.0e-7
    amp:
      enabled: true
      amp_dtype: float16
    gradient_clip:
      enabled: true
      max_norm: 0.1
      norm_type: 2

  # Scheduler configuration
  scheduler:
    name: "CosineAnnealingLR"
    enabled: true
    parameters:
      eta_min: 1.0e-7

  # Early stopping configuration
  early_stopping:
    enabled: false
    verbose: true
    patience: 30
    min_delta: 0.0
    monitor: "losses/val_core_loss"
    mode: "min"

  # Logging configuration
  logging:
    wandb:
      enabled: true
      project: "U-NET"
      entity: "svane20-keyshot"
      tags: [ "unet", "vgg16-bn", "synthetic-data" ]
      notes: "Training U-Net with VGG16-BN backbone on synthetic data."
      group: "unet-vgg16-bn"
      job_type: "training"
    log_directory: "./logs"
    log_metrics: false
    log_freq: 10

  # Checkpoint configuration
  checkpoint:
    save_directory: "D:/OneDrive/Master Thesis/ml/checkpoints/unet"
    save_freq: 5
    checkpoint_path: "unet_vgg16_bn_v1.pt"
    resume_from: null # "D:/OneDrive/Master Thesis/ml/checkpoints/unet/<CHECKPOINT>" or null

# Model configuration
model:
  image_encoder:
    pretrained: true
    freeze_pretrained: false
  mask_decoder:
    out_channels: 1
    dropout: 0.2