# @package _global_

# Data augmentation configuration
transforms:
  train:
    RandomAffine:
      degrees: 30
      translate: null
      scale: [ 0.8, 1.25 ]
      shear: 10
      flip: 0.5
      resample: false
      fillcolor: 0
    TopBiasedRandomCrop:
      output_size: [ 512, 512 ]
      top_crop_ratio: 0.4
      low_threshold: 0.1
      high_threshold: 0.9
    RandomJitter:
      enabled: true # Hack to show in wandb
    ToTensor:
      enabled: true # Hack to show in wandb
    Rescale:
      scale: 0.00392156862745098 # 1/255
    Normalize:
      mean: [ 0.485, 0.456, 0.406 ]
      std: [ 0.229, 0.224, 0.225 ]
      inplace: false
  val:
    OriginScale:
      output_size: 512
    ToTensor:
      enabled: true # Hack to show in wandb
    Rescale:
      scale: 0.00392156862745098 # 1/255
    Normalize:
      mean: [ 0.485, 0.456, 0.406 ]
      std: [ 0.229, 0.224, 0.225 ]
      inplace: false

# Dataset configuration
dataset:
  name: "synthetic-data"
  root: "/mnt/shared/datasets/processed"
  batch_size: 8
  pin_memory: true
  train:
    num_workers: 4
    shuffle: true
    drop_last: true
    use_trimap: false
    use_composition: false
  val:
    num_workers: 4
    shuffle: false
    drop_last: false

# Training configuration
training:
  max_epochs: 100
  warmup_epochs: 2
  accelerator: cuda
  seed: 42
  compile_model: true

  # Criterion configuration
  criterion:
    losses: [ "l1_loss", "gradient_loss", "laplacian_pha_loss" ]
    normalize_weights: true
    weight_dict:
      l1_loss: 1.0
      gradient_loss: 0.5
      laplacian_pha_loss: 0.5

  # Optimizer configuration
  optimizer:
    name: "AdamW"
    lr: 4e-4
    weight_decay: 1.0e-7
    amp:
      enabled: true
      amp_dtype: float16
    gradient_clip:
      enabled: true
      max_norm: 0.1
      norm_type: 2

  # Scheduler configuration
  scheduler:
    name: "CosineAnnealingLR"
    enabled: true
    parameters:
      eta_min: 1.0e-7

  # Early stopping configuration
  early_stopping:
    enabled: true
    verbose: true
    patience: 30
    min_delta: 0.0
    monitor: "mae"
    mode: "min"

  # Logging configuration
  logging:
    wandb:
      enabled: true
      project: "SWIN"
      entity: "svane20-keyshot"
      tags: [ "swin", "swin-small-patch4-window7-224", "synthetic-data" ]
      notes: "Training with swin-small-patch4-window7-224 backbone on synthetic data."
      group: "swin-small-patch4-window7-224"
      job_type: "training"
    log_metrics: false
    log_freq: 10
    log_images_freq: 10
    image_log_count: 8

  # Checkpoint configuration
  checkpoint:
    save_directory: "/mnt/shared/ml/checkpoints/swin"
    save_freq: 5
    checkpoint_path: "swin_small_patch4_window7_224_512_v1.pt"
    resume_from: null # "/mnt/shared/ml/checkpoints/swin/<CHECKPOINT>" or null

# Model configuration
model:
  model_name: "SwinMattingModel"
  encoder:
    model_name: "microsoft/swin-small-patch4-window7-224"
  decoder:
    use_attn: true
    refine_channels: 16